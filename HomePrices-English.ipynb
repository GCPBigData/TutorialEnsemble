{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial: Increasing the Predictive Power of Your Machine Learning Models with Stacking Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoever accompanies competitions knows that one of the most important things is to know how to put together several models to create a powerful solution. Several people have already asked me, by e-mail or in the presentations I made, about ensembles. This is an important issue not only for competitions, but also for real cases where you want to extract as much performance as possible from the models.\n",
    "\n",
    "Ensembles are sets of models that offer better performance than each model that composes it.\n",
    "\n",
    "So in this article I want to exemplify the best way I know of creating ensembles: stacking. This is a method I have used in all competitions that have had good results.\n",
    "\n",
    "Before you begin, a tip: It's important to think of Machine Learning as \"processes.\" Here we will not only test models in a database, but we will test processes, pipelines of methods applied to the data to know what the results are.\n",
    "\n",
    "This material is part of the presentation I will make (or did) on 30/11 at the PyData Meetup São Paulo. The original Jupyter Notebook is available at this link.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used are commercial real estate transactions in the city of Ames, Iowa. Our goal is to predict the selling price of a house by feeding the model with the features. This data is also the subject of a competition at Kaggle. The training and test data can be found at: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/\n",
    "\n",
    "Something very important is to explore the data for ideas of features and methods to validate the models. As the focus of this material is stacking, I'll skip this part. Nevertheless, I think it is important to clarify that we have two basic types of variables in this data: categorical and numerical.\n",
    "\n",
    "Categorical variables have levels that can not be ordered. In some cases it is possible to think of ways to order them, as a variable that describes whether a street is paved or not. In this case one can think that the paved street will value the property.\n",
    "\n",
    "Numeric variables can receive any continuous value. An example would be the size of the terrain.\n",
    "\n",
    "In this cell we load the data and create a DataFrame with the features (X) and a Series with the sale price (SalePrice, y) that is our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    MSSubClass MSZoning  LotFrontage  LotArea Street\n",
       "Id                                                  \n",
       "1           60       RL         65.0     8450   Pave\n",
       "2           20       RL         80.0     9600   Pave\n",
       "3           60       RL         68.0    11250   Pave\n",
       "4           70       RL         60.0     9550   Pave\n",
       "5           60       RL         84.0    14260   Pave"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv', index_col='Id')\n",
    "X, y = train.drop('SalePrice', axis=1), train.SalePrice.copy()\n",
    "train.head().iloc[:, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sets of Variables (Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to get different models is to vary the data representation used to train them. That is why our first step is to build these representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary Functions\n",
    "\n",
    "The metric suggested by Kaggle to evaluate the models is RMSLE, this error takes into account the difference between the logarithm of the predictions and the target. It is possible to think of this error as an approximation of the percentage error of the model, but with more interesting properties from the mathematical point of view.\n",
    "\n",
    "I created three functions to calculate the error. We are going to do transformations of the variable y, so to make our work easier, I decided to create custom functions for each transformation, thus avoiding some confusion when transforming them to compute the error in a single function.\n",
    "\n",
    "To use some Scikit-learn functions that will help keep our code cleaner and more efficient, we need to create the error functions in the way that the module requires. In this case, the pecse function receives a trained model, the features and the target. It computes the predictions and should return a number, which is the value of the error metric.\n",
    "\n",
    "The last cell line creates a cross-validation object from scikit-learn. He will take care of dividing the data for us. This subject deserves a number of articles, but basically it is a way to divide the data into N parts, and repeatedly use N-1 parts as training data, and the remaining part as test data. For example, if we have the parts [1, 2, 3, 4, 5], in one of the iterations we can train using parts [1, 2, 3, 4] and validate in part 5.\n",
    "\n",
    "Although this is a time series, Kaggle has decided to treat as independent random data, so we will do this simple cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def rmsle(estimator, X, y):\n",
    "    p = estimator.predict(X)\n",
    "    return np.sqrt(mean_squared_error(np.log1p(y), np.log1p(p)))\n",
    "\n",
    "def rmsle_log_y(estimator, X, y):\n",
    "    p = estimator.predict(X)\n",
    "    return np.sqrt(mean_squared_error(y, p))\n",
    "\n",
    "def rmsle_sqrt_y(estimator, X, y):\n",
    "    p = estimator.predict(X)\n",
    "    y = np.power(y, 2)\n",
    "    p = np.power(p, 2)\n",
    "    return np.sqrt(mean_squared_error(np.log1p(y), np.log1p(p)))\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature set 1: \"numeric\" variables\n",
    "\n",
    "The first set of features we will have will be a simple selection of the originally numeric data variables. To do this, just select all the columns that have variables with integers or floating point. Also, to replace the null values ​​(scikit-learn requires replacement), I decided to put the -1 value. Since we are going to use only tree-based models with this unique data, there is no need to worry too much about it for our purposes.\n",
    "\n",
    "After storing this data in variable X1, we see that there are 36 columns. Already in this part I want to train a model of Random Forest inside the cross-validation, to know how it would leave alone in the original data. For this I used the cross_val_score function. It is a feature that makes it much easier to cross-validate with scikit-learn. Just put a template and the data. In this case I chose to specify a validation scheme and a custom error metric.\n",
    "\n",
    "This function returns a list with the errors of each iteration of the cross validation, so I averaged to know the average error of the parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dims (1460, 36)\n",
      "RMSLE: 0.14582352618\n"
     ]
    }
   ],
   "source": [
    "X1 = X.select_dtypes(include=[np.number]).fillna(-1)\n",
    "print('Dims', X1.shape)\n",
    "model = RandomForestRegressor(n_estimators=1000, random_state=0)\n",
    "error = cross_val_score(model, X1, y, cv=kf, scoring=rmsle).mean()\n",
    "print('RMSLE:', error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature set 2: Ordinal Encoding Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create another set of features, this time adding the categorical variables. There are several ways to encode this type of variable for the models, one of which is using an ordinal format. This simply means replacing each original value with sequential numbers. On some models this can be problematic as they will try to capture some order relation in values they may not have. In our case, with models based on decision trees, this problem is almost non-existent.\n",
    "\n",
    "After coding in this way, we ran the cross-validation again, now on these new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dims (1460, 79)\n",
      "RMSLE: 0.143837364859\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X2 = X.copy()\n",
    "for col in X2.columns:\n",
    "    if X2[col].dtype == object:\n",
    "        enc = LabelEncoder()\n",
    "        X2[col] = enc.fit_transform(X[col].fillna('Missing'))\n",
    "\n",
    "print('Dims', X2.shape)\n",
    "X2.fillna(-1, inplace=True)\n",
    "model = RandomForestRegressor(n_estimators=1000, random_state=0)\n",
    "error = cross_val_score(model, X2, y, cv=kf, scoring=rmsle).mean()\n",
    "print('RMSLE:', error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: OneHot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most popular way to code categorical variables is One Hot Encoding. Basically consists of transforming each value of the variable into a column whose new value will be 1 if the variable has that value in a given example, or 0 if not. There are indications that decision trees do not process this kind of representation so well, but in some practical cases I've seen this work better than the ordinal, so look at this as another tool.\n",
    "\n",
    "This method creates more than 200 new columns, which makes the training process slower, so I decided to leave the cross-validation line commented out. If you want to see the result, just run it without the old woman's game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dims (1460, 288)\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.preprocessing import OneHotEncoder\n",
    "X3 = X.copy()\n",
    "cats = []\n",
    "for col in X3.columns:\n",
    "    if X3[col].dtype == object:\n",
    "        X3 = X3.join(pd.get_dummies(X3[col], prefix=col), how='left')\n",
    "        X3.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "\n",
    "print('Dims', X3.shape)\n",
    "X3.fillna(-1, inplace=True)\n",
    "model = RandomForestRegressor(n_estimators=1000, random_state=0)\n",
    "#cross_val_score(model, X3, y, cv=kf, scoring=rmsle).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Transformations\n",
    "\n",
    "An interesting way to create diversity, and sometimes even better performance, in a regression case is to transform the variable we are trying to predict. In this case we will test two transformations: logarithm and square root."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log\n",
    "\n",
    "It is possible to see that trying to predict the logarithm of the price gives us a better result. This happens not only because the model captures different patterns, but also because we use a metric based on the difference of logarithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, X1, log-target RMSLE: 0.14518580749\n",
      "RF, X2, log-target RMSLE: 0.14207134495\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor(n_estimators=1000, random_state=0)\n",
    "error = cross_val_score(model, X1, np.log1p(y), cv=kf, scoring=rmsle_log_y).mean()\n",
    "print('RF, X1, log-target RMSLE:', error)\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=1000, random_state=0)\n",
    "error = cross_val_score(model, X2, np.log1p(y), cv=kf, scoring=rmsle_log_y).mean()\n",
    "print('RF, X2, log-target RMSLE:', error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Square root\n",
    "\n",
    "This transformation also gives us a better result than using the variable in its original state. One of the suggestions of the reason we see this effect is that these transformations cause the variable y to have a distribution closer to normal, which facilitates the work of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, X1, sqrt-target RMSLE: 0.145652934484\n",
      "RF, X2, sqrt-target RMSLE: 0.143004600132\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor(n_estimators=1000, random_state=0)\n",
    "error = cross_val_score(model, X1, np.sqrt(y), cv=kf, scoring=rmsle_sqrt_y).mean()\n",
    "print('RF, X1, sqrt-target RMSLE:', error)\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=1000, random_state=0)\n",
    "error = cross_val_score(model, X2, np.sqrt(y), cv=kf, scoring=rmsle_sqrt_y).mean()\n",
    "print('RF, X2, sqrt-target RMSLE:', error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating models with different models / algorithms\n",
    "\n",
    "Another way to generate diversity for the ensemble is to generate different models. In this case I will use my preferred model, the GBM. This is also based on decision trees, but basically trains each tree sequentially focusing on the mistakes made by the previous ones.\n",
    "\n",
    "In the cells below you can see the performance of this model in the feature sets and transformations we use with Random Forest. We see that it brings a significant improvement, capturing better the patterns of the relationship between the variables and the sale price of the real estate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM, X1, log-target RMSLE: 0.133492454914\n",
      "GBM, X2, log-target RMSLE: 0.129806890482\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "                \n",
    "model = GradientBoostingRegressor(random_state=0)\n",
    "error = cross_val_score(model, X1, np.log1p(y), cv=kf, scoring=rmsle_log_y).mean()\n",
    "print('GBM, X1, log-target RMSLE:', error)\n",
    "\n",
    "model = GradientBoostingRegressor(random_state=0)\n",
    "error = cross_val_score(model, X2, np.log1p(y), cv=kf, scoring=rmsle_log_y).mean()\n",
    "print('GBM, X2, log-target RMSLE:', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM, X1, sqrt-target RMSLE: 0.134258972813\n",
      "GBM, X2, sqrt-target RMSLE: 0.130919235682\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "                \n",
    "model = GradientBoostingRegressor(random_state=0)\n",
    "error = cross_val_score(model, X1, np.sqrt(y), cv=kf, scoring=rmsle_sqrt_y).mean()\n",
    "print('GBM, X1, sqrt-target RMSLE:', error)\n",
    "\n",
    "model = GradientBoostingRegressor(random_state=0)\n",
    "error = cross_val_score(model, X2, np.sqrt(y), cv=kf, scoring=rmsle_sqrt_y).mean()\n",
    "print('GBM, X2, sqrt-target RMSLE:', error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the example and focus on the ensemble part, I will not adjust the hyperparameters of the model. Hyperparameters are the attributes of the model (such as the depth of the decision trees) that need to be adjusted using separate validation data or a cross-validation cycle.\n",
    "\n",
    "It is good to know that not always the best models form the best ensembles. It is important to have powerful models when stacking, but we must also remember that it is important to have diversity. Sometimes some models that have a higher error can capture different patterns of the best models and therefore contribute to the ensemble.\n",
    "\n",
    "If you decide to adjust the hyperparameters, it is important to place it within the cross-validation cycle that we will see in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "\n",
    "All we did above is so we can create our ensemble. This is the time to put together the methods used to improve the predictive power of our models.\n",
    "\n",
    "Stacking is a way of doing the ensemble in which we use models to make predictions, and then we use these predictions as features in new models, in what can be called the \"second level.\" You can do this process several times, but at each level the return on performance with respect to the required computation is less.\n",
    "\n",
    "In this phase we need two cycles of cross-validation: external and internal. Inside, we will train the models on the original data and make the predictions. On the outside, we will train the model using the first step predictions as features.\n",
    "\n",
    "At each step of internal cross validation, we will save the predictions for the part of the data that is used as validation. This way we will have predictions for all examples of our training data. In addition, we will train a model in the original training data so that we can make predictions for the test data.\n",
    "\n",
    "In the external cycle we will train a model in the predictions generated by the internal cycle, and the features of the validation data will be the predictions of the first level models in the test data.\n",
    "\n",
    "In our specific case, we will create predictions using all combinations of models (RF and GBM), target (log and square root) transformations and feature sets (X1 and X2). We will not use X3 because it would take a lot of time to train, and for the purposes of demonstrating the method these two will be enough.\n",
    "\n",
    "In the end we will have predictions of 8 models in the first level. In the second level I used a regularized linear regression (Ridge). Having these predictions we can compute the error in the data outside of our internal training and validation samples, which will give us a reliable estimate of the ensemble error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSLE Fold 0 - RMSLE 0.1248\n",
      "RMSLE Fold 1 - RMSLE 0.1449\n",
      "RMSLE Fold 2 - RMSLE 0.1257\n",
      "RMSLE Fold 3 - RMSLE 0.1409\n",
      "RMSLE Fold 4 - RMSLE 0.1087\n",
      "RMSLE CV5 0.1290\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "kf_out = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "kf_in = KFold(n_splits=5, shuffle=True, random_state=2)\n",
    "\n",
    "cv_mean = []\n",
    "for fold, (tr, ts) in enumerate(kf_out.split(X, y)):\n",
    "    X1_train, X1_test = X1.iloc[tr], X1.iloc[ts]\n",
    "    X2_train, X2_test = X2.iloc[tr], X2.iloc[ts]\n",
    "    y_train, y_test = y.iloc[tr], y.iloc[ts]\n",
    "    \n",
    "    modelos = [GradientBoostingRegressor(random_state=0), RandomForestRegressor(random_state=0)]\n",
    "    targets = [np.log1p, np.sqrt]\n",
    "    feature_sets = [(X1_train, X1_test), (X2_train, X2_test)]\n",
    "    \n",
    "    \n",
    "    predictions_cv = []\n",
    "    predictions_test = []\n",
    "    for model, target, feature_set in product(modelos, targets, feature_sets):\n",
    "        predictions_cv.append(cross_val_predict(model, feature_set[0], target(y_train), cv=kf_in).reshape(-1,1))\n",
    "        model.fit(feature_set[0], target(y_train))\n",
    "        ptest = model.predict(feature_set[1])\n",
    "        predictions_test.append(ptest.reshape(-1,1))\n",
    "    \n",
    "    predictions_cv = np.concatenate(predictions_cv, axis=1)\n",
    "    predictions_test = np.concatenate(predictions_test, axis=1)\n",
    "    \n",
    "    stacker = Ridge()\n",
    "    stacker.fit(predictions_cv, np.log1p(y_train))\n",
    "    error = rmsle_log_y(stacker, predictions_test, np.log1p(y_test))\n",
    "    cv_mean.append(error)\n",
    "    print('RMSLE Fold %d - RMSLE %.4f' % (fold, error))\n",
    "    \n",
    "print('RMSLE CV5 %.4f' % np.mean(cv_mean))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As we can see, our best first level model is the GBM trained with the log transformation in the X2 data, which reaches the error of 0.1288. Our ensemble reaches the value of 0,1290. An improvement of 0.62%.\n",
    "\n",
    "The purpose of this article was to demonstrate the method, without worrying too much about the end performance. An ensemble made for performance improvement may present a more significant result.\n",
    "\n",
    "In some cases, such as in investment funds or healthcare applications, a small improvement can have a very significant result in the real world, which justifies the creation of a more complete solution using stacking.\n",
    "\n",
    "As always in the Machine Learning application, nothing is a guarantee of success, but this method is one of the most consistent in offering an improvement."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
